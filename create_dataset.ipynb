{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "terminal-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data import get_tokenizer\n",
    "import re\n",
    "\n",
    "def yield_tokens(file_path, tokenizer):\n",
    "    with io.open(file_path, encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            if re.search('[a-zA-Z]', line):\n",
    "                yield tokenizer(line.lower().replace('\\n', '').strip())\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "small-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = \"data/discourses.mb.txt\"\n",
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "contains_chars = lambda x: re.search('[a-zA-Z]', x)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(textfile, tokenizer), specials=[\"<unk>\", \"<eos>\", \"<sos>\", \"<pad>\"])\n",
    "vocab.set_default_index(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "controlling-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "\n",
    "with io.open(textfile, encoding = 'utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        if contains_chars(line):\n",
    "            line = line.lower().replace('\\n', '').strip()\n",
    "            words = tokenizer(line)\n",
    "            tokens = [vocab[word] for word in words]\n",
    "            for i in range(1, len(tokens)): \n",
    "                n_gram_seqs = tokens[:i+1]\n",
    "                input_sequences.append(n_gram_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "improved-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(input_sequences)\n",
    "d_embedding = 32\n",
    "d_model = 32\n",
    "dropout = 0.1\n",
    "seq_len = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "working-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def padding(arr, seq_len, mode=\"right\"):\n",
    "    assert mode==\"right\" or mode==\"left\", \"invalid padding mode\"\n",
    "    if mode==\"right\":\n",
    "        return F.pad(arr, (0, seq_len-arr.nelement()))\n",
    "    else:\n",
    "        return F.pad(arr, (seq_len-arr.nelement()), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "instant-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "luc = nn.Embedding(num_tokens, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "oriented-lightning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 32])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = input_sequences[-1]\n",
    "example_padded = padding(torch.tensor(example), seq_len)\n",
    "embedding_func = lambda x: luc(torch.tensor(x, dtype=torch.long))\n",
    "example_embedding = torch.tensor([luc(token_num).tolist() for token_num in example_padded])\n",
    "example_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "middle-tuition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 32])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def positional_encoding(seq_len, d, n=10000):\n",
    "    p = np.zeros((seq_len, d))\n",
    "    for k in range(seq_len):\n",
    "        for i in np.arange(int(d/2)):\n",
    "            denominator = np.power(n, 2*i/d)\n",
    "            p[k, 2*i] = np.sin(k/denominator)\n",
    "            p[k, 2*i+1] = np.cos(k/denominator)\n",
    "    return p\n",
    "\n",
    "p_embedding = torch.tensor(positional_encoding(seq_len=1024, d=32, n=10000))\n",
    "p_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "indirect-sodium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 32])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep = torch.add(example_embedding, p_embedding)\n",
    "rep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-spirituality",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer :)",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
